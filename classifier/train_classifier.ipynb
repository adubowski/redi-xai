{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyfuipRdXQ5rdne4lhPG3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adubowski/redi-xai/blob/main/classifier/train_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFzu7KlCR4Pt"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from os.path import join as oj\n",
        "\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from numpy.random import randint\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import copy\n",
        "import gc\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ginv9jtpuO2V"
      },
      "source": [
        "### Mount Google Drive and create paths for directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrTOXk0lSC8F"
      },
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "dir_path = \"/content/drive/MyDrive/redi-detecting-cheating\"\n",
        "\n",
        "with open(oj(dir_path, 'config.json')) as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "model_path = oj(dir_path, data[\"model_folder\"], \"initial_classifier\")\n",
        "data_path = oj(dir_path, data[\"data_folder\"])\n",
        "\n",
        "seg_path  = oj(data_path, \"patch-segmentation\")\n",
        "not_cancer_path = oj(data_path, \"processed/no_cancer\")\n",
        "cancer_path = oj(data_path, \"processed/cancer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDKlrNh3ucu2"
      },
      "source": [
        "#### Arguments for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEo0TcCMUXXo"
      },
      "source": [
        "mean = np.asarray([0.485, 0.456, 0.406])\n",
        "std = np.asarray([0.229, 0.224, 0.225])\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "parser.add_argument('--batch_size', type=int, default=16, metavar='N',\n",
        "                    help='input batch size for training (default: 64)')\n",
        "\n",
        "parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
        "                    help='number of epochs to train (default: 10)')\n",
        "parser.add_argument('--lr', type=float, default=0.00001, metavar='LR',\n",
        "                    help='learning rate needs to be extremely small, otherwise loss nans (default: 0.00001)')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
        "                    help='SGD momentum (default: 0.5)')\n",
        "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
        "                    help='random seed (default: 42)')\n",
        "parser.add_argument('--regularizer_rate', type=float, default=0.0, metavar='N',\n",
        "                    help='hyperparameter for CDEP weight - higher means more regularization')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "\n",
        "num_epochs = args.epochs\n",
        "\n",
        "device = torch.device(0)\n",
        "\n",
        "torch.manual_seed(args.seed);\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "model.classifier[-1] = nn.Linear(4096, 2)\n",
        "model = model.to(device)\n",
        "params_to_update = model.classifier.parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqLnCkUugEZ"
      },
      "source": [
        "#### Functions for reading in the image files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1RZ5_CFZSqF"
      },
      "source": [
        "def load_folder(path):\n",
        "    list_files= os.listdir(path)\n",
        "    num_files = min([2000, len(list_files)])\n",
        "    imgs_np = np.empty((num_files,  299, 299,3))\n",
        "    for i in tqdm(range(num_files)):    # Take a max of 100 for purposes of testing the code.\n",
        "        try:\n",
        "            img = Image.open(oj(path, list_files[i]))\n",
        "            imgs_np[i] = np.asarray(img)/255.0\n",
        "            \n",
        "            img.close()\n",
        "        except:\n",
        "            print(i)\n",
        "    return imgs_np\n",
        "\n",
        "def load_seg(path, orig_path):\n",
        "    list_files= os.listdir(orig_path)\n",
        "    num_files = min([2000, len(list_files)])\n",
        "    imgs_np = np.zeros((num_files,  299, 299), dtype = np.bool)\n",
        "    for i in tqdm(range(num_files)):\n",
        "        if os.path.isfile(oj(path,  list_files[i])):\n",
        "            img = Image.open(oj(path, list_files[i]))\n",
        "            imgs_np[i] = np.asarray(img)[:,:,0] > 100\n",
        "            img.close()\n",
        "    return imgs_np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S6Ilm0xulka"
      },
      "source": [
        "#### Read in the malignant images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NfhilqzbFw1"
      },
      "source": [
        "cancer_set = load_folder(cancer_path)\n",
        "cancer_set -= mean[None, None, :]\n",
        "cancer_set /= std[None, None, :]\n",
        "\n",
        "cancer_targets = np.ones((cancer_set.shape[0])).astype(np.int64)\n",
        "\n",
        "cancer_dataset = TensorDataset(torch.from_numpy(cancer_set.swapaxes(1,3).swapaxes(2,2)).float(), torch.from_numpy(cancer_targets),torch.from_numpy(np.zeros((len(cancer_set), 299, 299), dtype = np.bool)))\n",
        "del cancer_set\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvb_gjDruphc"
      },
      "source": [
        "#### Read in the benign images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmFCxXF6Od8n"
      },
      "source": [
        "not_cancer_set = load_folder(not_cancer_path)\n",
        "not_cancer_set -= mean[None, None, :]\n",
        "not_cancer_set /= std[None, None, :]\n",
        "seg_set = load_seg(seg_path, not_cancer_path)\n",
        "\n",
        "not_cancer_targets = np.zeros((not_cancer_set.shape[0])).astype(np.int64)\n",
        "\n",
        "not_cancer_dataset = TensorDataset(torch.from_numpy(not_cancer_set.swapaxes(1,3).swapaxes(2,3)).float(), torch.from_numpy(not_cancer_targets),torch.from_numpy(seg_set))\n",
        "\n",
        "del not_cancer_set\n",
        "del seg_set\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNeCSZrmut9r"
      },
      "source": [
        "#### Combine datasets and split to train-test-val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSXkwWzEPB2B"
      },
      "source": [
        "complete_dataset = ConcatDataset((not_cancer_dataset, cancer_dataset ))\n",
        "num_total = len(complete_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_val = int(0.1 * num_total)\n",
        "num_test = num_total - num_train - num_val\n",
        "torch.manual_seed(0);\n",
        "train_dataset, test_dataset, val_dataset= torch.utils.data.random_split(complete_dataset, [num_train, num_test, num_val])\n",
        "datasets = {'train' : train_dataset, 'test':test_dataset, 'val': val_dataset}\n",
        "dataset_sizes = {'train' : len(train_dataset), 'test':len(test_dataset), 'val': len(val_dataset)}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'test','val']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAMQLRX5uytt"
      },
      "source": [
        "#### Weights for training\n",
        "Since the classes are unbalanced, we need to account for this in the loss function while training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL-6d9qHR3WP"
      },
      "source": [
        "cancer_ratio =len(cancer_dataset)/len(complete_dataset)\n",
        "\n",
        "not_cancer_ratio = 1- cancer_ratio\n",
        "cancer_weight = 1/cancer_ratio\n",
        "not_cancer_weight = 1/ not_cancer_ratio\n",
        "weights = np.asarray([not_cancer_weight, cancer_weight])\n",
        "weights /= weights.sum()\n",
        "weights = torch.tensor(weights).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = weights.double().float())\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyuCsMPBwBC2"
      },
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LomNLJdeR8mi"
      },
      "source": [
        "def gradient_sum(im, target, seg ,  model, crit, device='cuda'):\n",
        "    '''  assume that eveything is already on cuda'''\n",
        "    im.requires_grad = True\n",
        "    grad_params = torch.abs(torch.autograd.grad(crit(model(im), target), im,create_graph = True)[0].sum(dim=1).masked_select(seg.byte())**2).sum()\n",
        "    return grad_params\n",
        "\n",
        "def train_model(model,dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "    val_acc_history = []\n",
        "    val_loss_history = []\n",
        "    train_loss_history = []\n",
        "    \n",
        "    train_acc_history = []\n",
        "    train_cd_history= []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 10.0\n",
        "    patience = 3\n",
        "    cur_patience = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                optimizer.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_loss_cd = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for i, (inputs, labels, seg) in tqdm(enumerate(dataloaders[phase])):\n",
        "    \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                seg = seg.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # need to do calc beforehand because we do need the gradients\n",
        "                    if phase == 'train' and regularizer_rate !=0:\n",
        "                        inputs.requires_grad = True\n",
        "                        add_loss = gradient_sum(inputs, labels, seg, model, criterion)  \n",
        "                        if add_loss!=0:\n",
        "                            (regularizer_rate*add_loss).backward()\n",
        "                            optimizer.step()\n",
        "                        #print(torch.cuda.memory_allocated()/(np.power(10,9)))\n",
        "                        optimizer.zero_grad()   \n",
        "                        running_loss_cd +=add_loss.item() * inputs.size(0)\n",
        "     \n",
        "                        #inputs.require_grad = False\n",
        "                         \n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        (loss).backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_cd_loss = running_loss_cd / dataset_sizes[phase]\n",
        "       \n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "  \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f} CD Loss : {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc, epoch_cd_loss))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc.item())\n",
        "                val_loss_history.append(epoch_loss)\n",
        "            if phase == 'train':\n",
        "                train_loss_history.append(epoch_loss)\n",
        "                train_cd_history.append(epoch_cd_loss)\n",
        "                train_acc_history.append(epoch_acc.item())\n",
        "                \n",
        "            if phase == 'val':\n",
        "                if epoch_loss < best_loss:\n",
        "            \n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    cur_patience = 0\n",
        "                else:\n",
        "                    cur_patience+=1\n",
        "        if cur_patience >= patience:\n",
        "            break\n",
        "  \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "\n",
        "    hist_dict = {}\n",
        "    hist_dict['val_acc_history'] = val_acc_history\n",
        "    hist_dict['val_loss_history'] = val_loss_history\n",
        "    \n",
        "    hist_dict['train_acc_history'] = train_acc_history\n",
        "\n",
        "    hist_dict['train_loss_history'] = val_loss_history\n",
        "    hist_dict['train_cd_history'] = train_cd_history\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model,hist_dict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suYKwRYvv6DV"
      },
      "source": [
        "#### Train and save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt97ZEQcSARG"
      },
      "source": [
        "model, hist_dict = train_model(model, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)\n",
        "pid = ''.join([\"%s\" % randint(0, 9) for num in range(0, 20)])\n",
        "torch.save(model.classifier.state_dict(),oj(dir_path, model_path, pid + \".pt\"))\n",
        "\n",
        "hist_dict['pid'] = pid\n",
        "hist_dict['regularizer_rate'] = regularizer_rate\n",
        "hist_dict['seed'] = args.seed\n",
        "hist_dict['batch_size'] = args.batch_size\n",
        "hist_dict['learning_rate'] = args.lr\n",
        "hist_dict['momentum'] = args.momentum\n",
        "pkl.dump(hist_dict, open(os.path.join(model_path , pid +  '.pkl'), 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rs249a_wRv5"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "imshow(test_dataset[0][2], cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umEeCPdcz-Uu"
      },
      "source": [
        "input = test_dataset[0][0][None,:,:,:].to(device)\n",
        "model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCQBnNIxNiEc"
      },
      "source": [
        "from sklearn.metrics import auc,average_precision_score, roc_curve,roc_auc_score,precision_recall_curve, f1_score\n",
        "\n",
        "def get_output(model, dataset):\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
        "                                             shuffle=False, num_workers=4)\n",
        "    model = model.eval()\n",
        "    y = []\n",
        "    y_hat = []\n",
        "    softmax= torch.nn.Softmax()\n",
        "    with torch.no_grad() :\n",
        "        for inputs, labels, cd in data_loader:\n",
        "            y_hat.append((labels).cpu().numpy())\n",
        "            y.append(torch.nn.Softmax(dim=1)( model(inputs.cuda()))[:,1].detach().cpu().numpy()) # take the probability for cancer\n",
        "    y_hat = np.concatenate( y_hat, axis=0 )\n",
        "    y = np.concatenate( y, axis=0 )\n",
        "    return y, y_hat # in the training set the values were switched\n",
        "\n",
        "def get_auc_f1(model, dataset,fname = None, ):\n",
        "    if fname !=None:\n",
        "        with open(fname, 'rb') as f:\n",
        "            weights = torch.load(f)\n",
        "        if \"classifier.0.weight\" in weights.keys(): #for the gradient models we unfortunately saved all of the weights\n",
        "            model.load_state_dict(weights)\n",
        "        else:\n",
        "            model.classifier.load_state_dict(weights)\n",
        "        y, y_hat = get_output(model.classifier, dataset)\n",
        "    else:   \n",
        "        y, y_hat = get_output(model, dataset)\n",
        "    auc =roc_auc_score(y_hat, y)\n",
        "    f1 = np.asarray([f1_score(y_hat, y > x) for x in np.linspace(0.1,1, num = 10) if (y >x).any() and (y<x).any()]).max()\n",
        "    return auc, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7sSm3CHN1_T"
      },
      "source": [
        "get_auc_f1(model, test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qKwJHU1OJLc"
      },
      "source": [
        "out = get_output(model, test_dataset)\n",
        "\n",
        "print(np.where(out[0] > 0.5))\n",
        "print(np.where(out[1] == 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9RztNKPKERu"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73V2haOgLOYM"
      },
      "source": [
        "print(models.vgg16(pretrained=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}