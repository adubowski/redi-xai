{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uY8J4dOtaZQb",
        "yNqLnCkUugEZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adubowski/redi-xai/blob/main/classifier/train_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFzu7KlCR4Pt"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from os.path import join as oj\n",
        "from datetime import datetime\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from torch.utils.data import TensorDataset, ConcatDataset\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from numpy.random import randint\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import copy\n",
        "import gc\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ginv9jtpuO2V"
      },
      "source": [
        "### Mount Google Drive and create paths for directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrTOXk0lSC8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c44226-7753-4a1d-c205-2039f63f3ac3"
      },
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "dir_path = \"/content/drive/MyDrive/redi-detecting-cheating\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhfXd_ry4Og"
      },
      "source": [
        "with open(oj(dir_path, 'config.json')) as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "model_path = oj(dir_path, data[\"model_folder\"], \"initial_classifier\")\n",
        "model_training_path = oj(model_path, \"training\")\n",
        "data_path = oj(dir_path, data[\"data_folder\"])\n",
        "# seg_path  = oj(data_path, \"patch-segmentation\")\n",
        "not_cancer_path = oj(data_path, \"processed/no_cancer\")\n",
        "cancer_path = oj(data_path, \"processed/cancer\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDKlrNh3ucu2"
      },
      "source": [
        "#### Arguments for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEo0TcCMUXXo"
      },
      "source": [
        "mean = np.asarray([0.485, 0.456, 0.406])\n",
        "std = np.asarray([0.229, 0.224, 0.225])\n",
        "\n",
        "# Training settings\n",
        "parser = argparse.ArgumentParser(description='ISIC Lesion Classification')\n",
        "parser.add_argument('--batch_size', type=int, default=16, metavar='N',\n",
        "                    help='input batch size for training (default: 64)')\n",
        "\n",
        "parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
        "                    help='number of epochs to train (default: 10)')\n",
        "parser.add_argument('--lr', type=float, default=0.00001, metavar='LR',\n",
        "                    help='learning rate needs to be extremely small, otherwise loss nans (default: 0.00001)')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
        "                    help='SGD momentum (default: 0.5)')\n",
        "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
        "                    help='random seed (default: 42)')\n",
        "parser.add_argument('--regularizer_rate', type=float, default=0.0, metavar='N',\n",
        "                    help='hyperparameter for CDEP weight - higher means more regularization')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "regularizer_rate = args.regularizer_rate\n",
        "\n",
        "num_epochs = args.epochs\n",
        "\n",
        "device = torch.device(0)\n",
        "\n",
        "torch.manual_seed(args.seed);\n",
        "model = models.vgg16(pretrained=True)\n",
        "# model = torch.hub.load('pytorch/vision:v0.9.0', 'inception_v3', pretrained=True)\n",
        "\n",
        "model.classifier[-1] = nn.Linear(4096, 2)\n",
        "model = model.to(device)\n",
        "params_to_update = model.classifier.parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY8J4dOtaZQb"
      },
      "source": [
        "#### Remove empty images from the directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNzmHhf0aYB6"
      },
      "source": [
        "# def clean_up_path(path):\n",
        "#     list_files= os.listdir(path)\n",
        "#     num_files = len(list_files)\n",
        "#     for i in tqdm(range(num_files)):\n",
        "#         if os.path.getsize(oj(path, list_files[i])) < 100:\n",
        "#             os.remove(oj(path, list_files[i]))\n",
        "#             print(\"File \" + str(i) + \"deleted!\")\n",
        "\n",
        "# clean_up_path(cancer_path)\n",
        "# clean_up_path(not_cancer_path)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqLnCkUugEZ"
      },
      "source": [
        "#### Reading in the image files (unused)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1RZ5_CFZSqF"
      },
      "source": [
        "# def load_seg(path, orig_path):\n",
        "#     list_files= os.listdir(orig_path)\n",
        "#     num_files = min([2000, len(list_files)])\n",
        "#     imgs_np = np.zeros((num_files,  299, 299), dtype = np.bool)\n",
        "#     for i in tqdm(range(num_files)):\n",
        "#         if os.path.isfile(oj(path,  list_files[i])):\n",
        "#             img = Image.open(oj(path, list_files[i]))\n",
        "#             imgs_np[i] = np.asarray(img)[:,:,0] > 100\n",
        "#             img.close()\n",
        "#     return imgs_np\n",
        "\n",
        "# cancer_set = load_folder(cancer_path)\n",
        "# cancer_set -= mean[None, None, :]\n",
        "# cancer_set /= std[None, None, :]\n",
        "\n",
        "# cancer_targets = np.ones((cancer_set.shape[0])).astype(np.bool)\n",
        "\n",
        "# cancer_dataset = TensorDataset(torch.from_numpy(\n",
        "#     cancer_set.swapaxes(1,3).swapaxes(2,2)).float(), \n",
        "#     torch.from_numpy(cancer_targets), \n",
        "#     torch.from_numpy(np.zeros((len(cancer_set), 299, 299), dtype = np.bool))\n",
        "# )\n",
        "# del cancer_set\n",
        "\n",
        "# gc.collect()\n",
        "\n",
        "# not_cancer_set = load_folder(not_cancer_path)\n",
        "# not_cancer_set -= mean[None, None, :]\n",
        "# not_cancer_set /= std[None, None, :]\n",
        "# seg_set = load_seg(seg_path, not_cancer_path)\n",
        "\n",
        "# not_cancer_targets = np.zeros((not_cancer_set.shape[0])).astype(np.bool)\n",
        "\n",
        "# not_cancer_dataset = TensorDataset(torch.from_numpy(not_cancer_set.swapaxes(1,3).swapaxes(2,3)).float(), torch.from_numpy(not_cancer_targets),torch.from_numpy(seg_set))\n",
        "\n",
        "# del not_cancer_set\n",
        "# del seg_set\n",
        "\n",
        "# gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7by8ywuLarvl"
      },
      "source": [
        "#### Torch dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdF2_RMJmLY2"
      },
      "source": [
        "class CancerDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path: str, is_cancer: int):\n",
        "        self.path = path\n",
        "        self.data_files = os.listdir(self.path)\n",
        "        self.is_cancer = is_cancer\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = Image.open(oj(self.path, self.data_files[i]))\n",
        "        img_array = np.asarray(img)/255.0\n",
        "        img_array -= mean[None, None, :]\n",
        "        img_array /= std[None, None, :]\n",
        "        img.close()\n",
        "        torch_img = torch.from_numpy(img_array.swapaxes(0,2).swapaxes(1,2)).float()\n",
        "        return (torch_img, self.is_cancer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNeCSZrmut9r"
      },
      "source": [
        "#### Combine datasets and split to train-test-val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSXkwWzEPB2B"
      },
      "source": [
        "cancer_dataset = CancerDataset(cancer_path, 1)\n",
        "not_cancer_dataset = CancerDataset(not_cancer_path, 0)\n",
        "complete_dataset = ConcatDataset((cancer_dataset, not_cancer_dataset))\n",
        "num_total = len(complete_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_val = int(0.1 * num_total)\n",
        "num_test = num_total - num_train - num_val\n",
        "torch.manual_seed(0);\n",
        "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(complete_dataset, [num_train, num_test, num_val])\n",
        "datasets = {'train' : train_dataset, 'test':test_dataset, 'val': val_dataset}\n",
        "dataset_sizes = {'train' : len(train_dataset), 'test':len(test_dataset), 'val': len(val_dataset)}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=args.batch_size,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test','val']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as36MDxbJFsa"
      },
      "source": [
        "def list_to_file(li, filename):\n",
        "  with open(filename, 'w') as f:\n",
        "    for item in li:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "\n",
        "def extract_filenames(train_subset, val_subset, test_subset):\n",
        "  # Extract the relevant indices of the concat dataset\n",
        "  train_idx, val_idx, test_idx = train_subset.indices, val_subset.indices, test_subset.indices\n",
        "\n",
        "  # Extract the filenames for the cancer_dataset and not_cancer_dataset and concatenate with their directory path.\n",
        "  # Each original dataset is stored by the ConcatDataset class. So even though train_subset is a subset, the info for the whole cancer dataset is stored in train_subset.dataset.datasets[0]\n",
        "  cancer_filepaths      = [oj(train_subset.dataset.datasets[0].path, file) for file in train_subset.dataset.datasets[0].data_files]\n",
        "  not_cancer_filepaths  = [oj(train_subset.dataset.datasets[1].path, file) for file in train_subset.dataset.datasets[1].data_files]\n",
        "\n",
        "  filepaths = cancer_filepaths + not_cancer_filepaths    # Append the lists together, this combined list is what the indices are based on.\n",
        "\n",
        "  train_files = [filepaths[i] for i in train_idx]\n",
        "  val_files   = [filepaths[i] for i in val_idx] \n",
        "  test_files  = [filepaths[i] for i in test_idx]\n",
        "\n",
        "  return train_files, val_files, test_files\n",
        "\n",
        "# Call the function and get the full file paths.\n",
        "train_files, val_files, test_files = extract_filenames(train_dataset, val_dataset, test_dataset)\n",
        "\n",
        "list_to_file(train_files, oj(dir_path, 'models', 'train_files.txt'))   # Write the training filepaths to a text file.\n",
        "list_to_file(val_files,   oj(dir_path, 'models', 'val_files.txt'))     # Write the validation filepaths to a text file.\n",
        "list_to_file(test_files,  oj(dir_path, 'models', 'test_files.txt'))    # Write the testing filepaths to a text file."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAMQLRX5uytt"
      },
      "source": [
        "#### Weights for training\n",
        "Since the classes are unbalanced, we need to account for this in the loss function while training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL-6d9qHR3WP"
      },
      "source": [
        "cancer_ratio = len(cancer_dataset)/len(complete_dataset)\n",
        "\n",
        "not_cancer_ratio = 1 - cancer_ratio\n",
        "cancer_weight = 1/cancer_ratio\n",
        "not_cancer_weight = 1/ not_cancer_ratio\n",
        "weights = np.asarray([not_cancer_weight, cancer_weight])\n",
        "weights /= weights.sum()\n",
        "weights = torch.tensor(weights).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = weights.double().float())\n",
        "\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=args.lr, momentum=args.momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyuCsMPBwBC2"
      },
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LomNLJdeR8mi"
      },
      "source": [
        "def gradient_sum(im, target, model, crit, device='cuda'):\n",
        "    '''  assume that eveything is already on cuda'''\n",
        "    im.requires_grad = True\n",
        "    grad_params = torch.abs(torch.autograd.grad(crit(model(im), target), im,create_graph = True)[0].sum(dim=1)).sum()\n",
        "    return grad_params\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "    val_acc_history = []\n",
        "    val_loss_history = []\n",
        "    train_loss_history = []\n",
        "    \n",
        "    train_acc_history = []\n",
        "    train_cd_history= []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 10.0\n",
        "    patience = 3\n",
        "    cur_patience = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        if len(os.listdir(model_training_path)) > 0:\n",
        "          model_list = [(f, os.path.getmtime(oj(model_training_path,f))) for f in os.listdir(model_training_path) if f.endswith('.pt')]\n",
        "          model_list.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place from most to least recent\n",
        "          model_name = model_list[0][0]\n",
        "          model.classifier.load_state_dict(torch.load(oj(model_training_path, model_name)))\n",
        "          print(\"Model loaded! Epoch: \", epoch)\n",
        "          model.eval()\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                optimizer.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_loss_cd = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for i, (inputs, labels) in tqdm(enumerate(dataloaders[phase])):\n",
        "    \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # need to do calc beforehand because we do need the gradients\n",
        "                    if phase == 'train' and regularizer_rate !=0:\n",
        "                        inputs.requires_grad = True\n",
        "                        add_loss = gradient_sum(inputs, labels, model, criterion)  \n",
        "                        if add_loss!=0:\n",
        "                            (regularizer_rate*add_loss).backward()\n",
        "                            optimizer.step()\n",
        "                        #print(torch.cuda.memory_allocated()/(np.power(10,9)))\n",
        "                        optimizer.zero_grad()   \n",
        "                        running_loss_cd +=add_loss.item() * inputs.size(0)\n",
        "     \n",
        "                        #inputs.require_grad = False\n",
        "                         \n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        (loss).backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_cd_loss = running_loss_cd / dataset_sizes[phase]\n",
        "       \n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "  \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f} CD Loss : {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc, epoch_cd_loss))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc.item())\n",
        "                val_loss_history.append(epoch_loss)\n",
        "            if phase == 'train':\n",
        "                train_loss_history.append(epoch_loss)\n",
        "                train_cd_history.append(epoch_cd_loss)\n",
        "                train_acc_history.append(epoch_acc.item())\n",
        "                torch.save(model.classifier.state_dict(), oj(model_training_path, datetime.now().strftime(\"%Y%m%d%H%M%S\") + \".pt\"))     \n",
        "            if phase == 'val':\n",
        "                if epoch_loss < best_loss:\n",
        "            \n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    cur_patience = 0\n",
        "                else:\n",
        "                    cur_patience+=1\n",
        "        if cur_patience >= patience:\n",
        "            break\n",
        "  \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "\n",
        "    hist_dict = {}\n",
        "    hist_dict['val_acc_history'] = val_acc_history\n",
        "    hist_dict['val_loss_history'] = val_loss_history\n",
        "    \n",
        "    hist_dict['train_acc_history'] = train_acc_history\n",
        "\n",
        "    hist_dict['train_loss_history'] = val_loss_history\n",
        "    hist_dict['train_cd_history'] = train_cd_history\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, hist_dict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suYKwRYvv6DV"
      },
      "source": [
        "#### Train and save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt97ZEQcSARG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f18654-cf79-419c-8051-5f47a460fcf6"
      },
      "source": [
        "model, hist_dict = train_model(model, dataloaders, criterion, optimizer_ft, num_epochs=num_epochs)\n",
        "pid = datetime.now().strftime('%Y%m%d%H%M%S') #''.join([\"%s\" % randint(0, 9) for num in range(0, 20)])\n",
        "torch.save(model.classifier.state_dict(),oj(dir_path, model_path, pid + \".pt\"))\n",
        "\n",
        "hist_dict['pid'] = pid\n",
        "hist_dict['regularizer_rate'] = regularizer_rate\n",
        "hist_dict['seed'] = args.seed\n",
        "hist_dict['batch_size'] = args.batch_size\n",
        "hist_dict['learning_rate'] = args.lr\n",
        "hist_dict['momentum'] = args.momentum\n",
        "pkl.dump(hist_dict, open(os.path.join(model_path , pid +  '.pkl'), 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/4\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:10, 10.57s/it]\u001b[A\n",
            "2it [00:11,  7.72s/it]\u001b[A\n",
            "3it [00:20,  8.15s/it]\u001b[A\n",
            "4it [00:23,  6.38s/it]\u001b[A\n",
            "5it [00:31,  7.12s/it]\u001b[A\n",
            "6it [00:32,  5.30s/it]\u001b[A\n",
            "7it [00:41,  6.22s/it]\u001b[A\n",
            "8it [00:42,  4.81s/it]\u001b[A\n",
            "9it [00:52,  6.35s/it]\u001b[A\n",
            "10it [00:53,  4.72s/it]\u001b[A\n",
            "11it [01:01,  5.77s/it]\u001b[A\n",
            "12it [01:03,  4.45s/it]\u001b[A\n",
            "13it [01:10,  5.40s/it]\u001b[A\n",
            "14it [01:12,  4.35s/it]\u001b[A\n",
            "15it [01:21,  5.66s/it]\u001b[A\n",
            "16it [01:22,  4.24s/it]\u001b[A\n",
            "17it [01:30,  5.43s/it]\u001b[A\n",
            "18it [01:32,  4.23s/it]\u001b[A\n",
            "19it [01:39,  5.09s/it]\u001b[A\n",
            "20it [01:41,  4.16s/it]\u001b[A\n",
            "21it [01:50,  5.77s/it]\u001b[A\n",
            "22it [01:51,  4.32s/it]\u001b[A\n",
            "23it [02:01,  5.96s/it]\u001b[A\n",
            "24it [02:02,  4.61s/it]\u001b[A\n",
            "25it [02:13,  6.51s/it]\u001b[A\n",
            "26it [02:14,  4.83s/it]\u001b[A\n",
            "27it [02:23,  6.07s/it]\u001b[A\n",
            "28it [02:24,  4.53s/it]\u001b[A\n",
            "29it [02:34,  6.00s/it]\u001b[A\n",
            "30it [02:34,  4.48s/it]\u001b[A\n",
            "31it [02:45,  6.25s/it]\u001b[A\n",
            "32it [02:46,  4.65s/it]\u001b[A\n",
            "33it [02:56,  6.39s/it]\u001b[A\n",
            "34it [02:57,  4.75s/it]\u001b[A\n",
            "35it [03:07,  6.22s/it]\u001b[A\n",
            "36it [03:08,  4.64s/it]\u001b[A\n",
            "37it [03:19,  6.53s/it]\u001b[A\n",
            "38it [03:20,  4.85s/it]\u001b[A\n",
            "39it [03:27,  5.62s/it]\u001b[A\n",
            "40it [03:29,  4.45s/it]\u001b[A\n",
            "41it [03:39,  6.19s/it]\u001b[A\n",
            "42it [03:40,  4.61s/it]\u001b[A\n",
            "43it [03:49,  5.93s/it]\u001b[A\n",
            "44it [03:50,  4.43s/it]\u001b[A\n",
            "45it [04:01,  6.31s/it]\u001b[A\n",
            "46it [04:01,  4.69s/it]\u001b[A\n",
            "47it [04:13,  6.72s/it]\u001b[A\n",
            "48it [04:14,  4.98s/it]\u001b[A\n",
            "49it [04:23,  6.35s/it]\u001b[A\n",
            "50it [04:24,  4.72s/it]\u001b[A\n",
            "51it [04:34,  6.16s/it]\u001b[A\n",
            "52it [04:35,  4.59s/it]\u001b[A\n",
            "53it [04:45,  6.22s/it]\u001b[A\n",
            "54it [04:46,  4.64s/it]\u001b[A\n",
            "55it [04:55,  5.96s/it]\u001b[A\n",
            "56it [04:56,  4.45s/it]\u001b[A\n",
            "57it [05:04,  5.75s/it]\u001b[A\n",
            "58it [05:05,  4.30s/it]\u001b[A\n",
            "59it [05:15,  5.82s/it]\u001b[A\n",
            "60it [05:16,  4.36s/it]\u001b[A\n",
            "61it [05:27,  6.31s/it]\u001b[A\n",
            "62it [05:28,  4.69s/it]\u001b[A\n",
            "63it [05:37,  6.00s/it]\u001b[A\n",
            "64it [05:37,  4.48s/it]\u001b[A\n",
            "65it [05:46,  5.82s/it]\u001b[A\n",
            "66it [05:47,  4.35s/it]\u001b[A\n",
            "67it [05:56,  5.73s/it]\u001b[A\n",
            "68it [05:57,  4.29s/it]\u001b[A\n",
            "69it [06:07,  6.06s/it]\u001b[A\n",
            "70it [06:08,  4.52s/it]\u001b[A\n",
            "71it [06:17,  5.82s/it]\u001b[A\n",
            "72it [06:18,  4.35s/it]\u001b[A\n",
            "73it [06:27,  5.78s/it]\u001b[A\n",
            "74it [06:28,  4.33s/it]\u001b[A\n",
            "75it [06:38,  6.07s/it]\u001b[A\n",
            "76it [06:39,  4.53s/it]\u001b[A\n",
            "77it [06:48,  5.87s/it]\u001b[A\n",
            "78it [06:49,  4.39s/it]\u001b[A\n",
            "79it [07:00,  6.30s/it]\u001b[A\n",
            "80it [07:01,  4.69s/it]\u001b[A\n",
            "81it [07:10,  5.93s/it]\u001b[A\n",
            "82it [07:11,  4.43s/it]\u001b[A\n",
            "83it [07:22,  6.39s/it]\u001b[A\n",
            "84it [07:23,  4.75s/it]\u001b[A\n",
            "85it [07:34,  6.70s/it]\u001b[A\n",
            "86it [07:35,  4.97s/it]\u001b[A\n",
            "87it [07:45,  6.46s/it]\u001b[A\n",
            "88it [07:46,  4.81s/it]\u001b[A\n",
            "89it [07:57,  6.82s/it]\u001b[A\n",
            "90it [07:58,  5.05s/it]\u001b[A\n",
            "91it [08:08,  6.44s/it]\u001b[A\n",
            "92it [08:09,  4.79s/it]\u001b[A\n",
            "93it [08:19,  6.48s/it]\u001b[A\n",
            "94it [08:20,  4.81s/it]\u001b[A\n",
            "95it [08:30,  6.37s/it]\u001b[A\n",
            "96it [08:31,  4.74s/it]\u001b[A\n",
            "97it [08:40,  5.91s/it]\u001b[A\n",
            "98it [08:41,  4.42s/it]\u001b[A\n",
            "99it [08:50,  5.98s/it]\u001b[A\n",
            "100it [08:51,  4.47s/it]\u001b[A\n",
            "101it [09:01,  6.19s/it]\u001b[A\n",
            "102it [09:02,  4.61s/it]\u001b[A\n",
            "103it [09:11,  5.87s/it]\u001b[A\n",
            "104it [09:12,  4.39s/it]\u001b[A\n",
            "105it [09:23,  6.48s/it]\u001b[A\n",
            "106it [09:24,  4.81s/it]\u001b[A\n",
            "107it [09:34,  6.20s/it]\u001b[A\n",
            "108it [09:35,  4.62s/it]\u001b[A\n",
            "109it [09:45,  6.21s/it]\u001b[A\n",
            "110it [09:45,  4.63s/it]\u001b[A\n",
            "111it [09:55,  6.22s/it]\u001b[A\n",
            "112it [09:56,  4.63s/it]\u001b[A\n",
            "113it [10:06,  6.04s/it]\u001b[A\n",
            "114it [10:07,  4.51s/it]\u001b[A\n",
            "115it [10:17,  6.28s/it]\u001b[A\n",
            "116it [10:18,  4.68s/it]\u001b[A\n",
            "117it [10:27,  5.99s/it]\u001b[A\n",
            "118it [10:28,  4.47s/it]\u001b[A\n",
            "119it [10:38,  6.13s/it]\u001b[A\n",
            "120it [10:39,  4.57s/it]\u001b[A\n",
            "121it [10:49,  6.09s/it]\u001b[A\n",
            "122it [10:49,  4.55s/it]\u001b[A\n",
            "123it [11:00,  6.46s/it]\u001b[A\n",
            "124it [11:01,  4.80s/it]\u001b[A\n",
            "125it [11:10,  6.12s/it]\u001b[A\n",
            "126it [11:11,  4.56s/it]\u001b[A\n",
            "127it [11:22,  6.36s/it]\u001b[A\n",
            "128it [11:23,  4.73s/it]\u001b[A\n",
            "129it [11:33,  6.40s/it]\u001b[A\n",
            "130it [11:34,  4.76s/it]\u001b[A\n",
            "131it [11:43,  5.98s/it]\u001b[A\n",
            "132it [11:44,  4.46s/it]\u001b[A\n",
            "133it [11:56,  6.72s/it]\u001b[A\n",
            "134it [11:57,  4.99s/it]\u001b[A\n",
            "135it [12:07,  6.67s/it]\u001b[A\n",
            "136it [12:08,  4.95s/it]\u001b[A\n",
            "137it [12:17,  6.09s/it]\u001b[A\n",
            "138it [12:18,  4.54s/it]\u001b[A\n",
            "139it [12:29,  6.37s/it]\u001b[A\n",
            "140it [12:30,  4.74s/it]\u001b[A\n",
            "141it [12:40,  6.33s/it]\u001b[A\n",
            "142it [12:41,  4.71s/it]\u001b[A\n",
            "143it [12:49,  5.96s/it]\u001b[A\n",
            "144it [12:50,  4.45s/it]\u001b[A\n",
            "145it [13:00,  5.85s/it]\u001b[A\n",
            "146it [13:00,  4.38s/it]\u001b[A\n",
            "147it [13:11,  6.19s/it]\u001b[A\n",
            "148it [13:12,  4.61s/it]\u001b[A\n",
            "149it [13:21,  5.90s/it]\u001b[A\n",
            "150it [13:22,  4.41s/it]\u001b[A\n",
            "151it [13:33,  6.39s/it]\u001b[A\n",
            "152it [13:34,  4.76s/it]\u001b[A\n",
            "153it [13:42,  5.79s/it]\u001b[A\n",
            "154it [13:43,  4.33s/it]\u001b[A\n",
            "155it [13:51,  5.48s/it]\u001b[A\n",
            "156it [13:52,  4.12s/it]\u001b[A\n",
            "157it [14:02,  5.93s/it]\u001b[A\n",
            "158it [14:03,  4.43s/it]\u001b[A\n",
            "159it [14:12,  5.93s/it]\u001b[A\n",
            "160it [14:13,  4.43s/it]\u001b[A\n",
            "161it [14:21,  5.34s/it]\u001b[A\n",
            "162it [14:22,  4.02s/it]\u001b[A\n",
            "163it [14:32,  5.94s/it]\u001b[A\n",
            "164it [14:33,  4.44s/it]\u001b[A\n",
            "165it [14:42,  5.76s/it]\u001b[A\n",
            "166it [14:43,  4.31s/it]\u001b[A\n",
            "167it [14:54,  6.26s/it]\u001b[A\n",
            "168it [14:55,  4.66s/it]\u001b[A\n",
            "169it [15:05,  6.31s/it]\u001b[A\n",
            "170it [15:06,  4.70s/it]\u001b[A\n",
            "171it [15:15,  5.97s/it]\u001b[A\n",
            "172it [15:15,  4.46s/it]\u001b[A\n",
            "173it [15:25,  5.94s/it]\u001b[A\n",
            "174it [15:26,  4.44s/it]\u001b[A\n",
            "175it [15:35,  5.76s/it]\u001b[A\n",
            "176it [15:36,  4.32s/it]\u001b[A\n",
            "177it [15:43,  5.21s/it]\u001b[A\n",
            "178it [15:44,  3.93s/it]\u001b[A\n",
            "179it [15:54,  5.68s/it]\u001b[A\n",
            "180it [15:55,  4.25s/it]\u001b[A\n",
            "181it [16:02,  5.33s/it]\u001b[A\n",
            "182it [16:03,  4.01s/it]\u001b[A\n",
            "183it [16:13,  5.59s/it]\u001b[A\n",
            "184it [16:14,  4.20s/it]\u001b[A\n",
            "185it [16:22,  5.63s/it]\u001b[A\n",
            "186it [16:24,  4.37s/it]\u001b[A\n",
            "187it [16:33,  5.70s/it]\u001b[A\n",
            "188it [16:34,  4.40s/it]\u001b[A\n",
            "189it [16:40,  4.73s/it]\u001b[A\n",
            "190it [16:46,  5.25s/it]\u001b[A\n",
            "191it [16:51,  5.04s/it]\u001b[A\n",
            "192it [16:57,  5.44s/it]\u001b[A\n",
            "193it [17:00,  4.64s/it]\u001b[A\n",
            "194it [17:08,  5.73s/it]\u001b[A\n",
            "195it [17:09,  4.29s/it]\u001b[A\n",
            "196it [17:19,  5.97s/it]\u001b[A\n",
            "197it [17:20,  4.47s/it]\u001b[A\n",
            "198it [17:27,  5.29s/it]\u001b[A\n",
            "199it [17:28,  3.98s/it]\u001b[A\n",
            "200it [17:37,  5.44s/it]\u001b[A\n",
            "201it [17:38,  4.19s/it]\u001b[A\n",
            "202it [17:47,  5.48s/it]\u001b[A\n",
            "203it [17:49,  4.69s/it]\u001b[A\n",
            "204it [17:54,  4.63s/it]\u001b[A\n",
            "205it [18:01,  5.29s/it]\u001b[A\n",
            "206it [18:04,  4.81s/it]\u001b[A\n",
            "207it [18:10,  4.96s/it]\u001b[A\n",
            "208it [18:15,  4.92s/it]\u001b[A\n",
            "209it [18:21,  5.32s/it]\u001b[A\n",
            "210it [18:25,  5.00s/it]\u001b[A\n",
            "211it [18:31,  5.21s/it]\u001b[A\n",
            "212it [18:36,  5.11s/it]\u001b[A\n",
            "213it [18:41,  5.28s/it]\u001b[A\n",
            "214it [18:45,  4.72s/it]\u001b[A\n",
            "215it [18:48,  4.32s/it]\u001b[A\n",
            "216it [18:55,  5.03s/it]\u001b[A\n",
            "217it [18:59,  4.65s/it]\u001b[A\n",
            "218it [19:05,  5.18s/it]\u001b[A\n",
            "219it [19:10,  5.21s/it]\u001b[A\n",
            "220it [19:15,  5.13s/it]\u001b[A\n",
            "221it [19:22,  5.53s/it]\u001b[A\n",
            "222it [19:26,  5.12s/it]\u001b[A\n",
            "223it [19:33,  5.74s/it]\u001b[A\n",
            "224it [19:36,  5.03s/it]\u001b[A\n",
            "225it [19:43,  5.65s/it]\u001b[A\n",
            "226it [19:48,  5.21s/it]\u001b[A\n",
            "227it [19:53,  5.10s/it]\u001b[A\n",
            "228it [19:58,  5.09s/it]\u001b[A\n",
            "229it [20:04,  5.43s/it]\u001b[A\n",
            "230it [20:07,  4.83s/it]\u001b[A\n",
            "231it [20:13,  5.19s/it]\u001b[A\n",
            "232it [20:18,  4.96s/it]\u001b[A\n",
            "233it [20:24,  5.49s/it]\u001b[A\n",
            "234it [20:29,  5.22s/it]\u001b[A\n",
            "235it [20:35,  5.49s/it]\u001b[A\n",
            "236it [20:36,  4.23s/it]\u001b[A\n",
            "237it [20:45,  5.46s/it]\u001b[A\n",
            "238it [20:46,  4.10s/it]\u001b[A\n",
            "239it [20:57,  6.32s/it]\u001b[A\n",
            "240it [20:58,  4.70s/it]\u001b[A\n",
            "241it [21:08,  6.14s/it]\u001b[A\n",
            "242it [21:09,  4.58s/it]\u001b[A\n",
            "243it [21:16,  5.36s/it]\u001b[A\n",
            "244it [21:17,  4.03s/it]\u001b[A\n",
            "245it [21:26,  5.60s/it]\u001b[A\n",
            "246it [21:27,  4.20s/it]\u001b[A\n",
            "247it [21:38,  6.30s/it]\u001b[A\n",
            "248it [21:39,  4.69s/it]\u001b[A\n",
            "249it [21:49,  6.41s/it]\u001b[A\n",
            "250it [21:50,  4.77s/it]\u001b[A\n",
            "251it [22:01,  6.64s/it]\u001b[A\n",
            "252it [22:02,  4.93s/it]\u001b[A\n",
            "253it [22:13,  6.56s/it]\u001b[A\n",
            "254it [22:14,  4.88s/it]\u001b[A\n",
            "255it [22:22,  5.85s/it]\u001b[A\n",
            "256it [22:23,  4.38s/it]\u001b[A\n",
            "257it [22:33,  6.22s/it]\u001b[A\n",
            "258it [22:34,  4.64s/it]\u001b[A\n",
            "259it [22:45,  6.46s/it]\u001b[A\n",
            "260it [22:46,  4.80s/it]\u001b[A\n",
            "261it [22:54,  5.87s/it]\u001b[A\n",
            "262it [22:55,  4.39s/it]\u001b[A\n",
            "263it [23:06,  6.35s/it]\u001b[A\n",
            "264it [23:07,  4.73s/it]\u001b[A\n",
            "265it [23:17,  6.46s/it]\u001b[A\n",
            "266it [23:18,  4.81s/it]\u001b[A\n",
            "267it [23:28,  6.33s/it]\u001b[A\n",
            "268it [23:29,  4.71s/it]\u001b[A\n",
            "269it [23:40,  6.46s/it]\u001b[A\n",
            "270it [23:41,  4.80s/it]\u001b[A\n",
            "271it [23:51,  6.53s/it]\u001b[A\n",
            "272it [23:52,  4.85s/it]\u001b[A\n",
            "273it [24:02,  6.36s/it]\u001b[A\n",
            "274it [24:03,  4.73s/it]\u001b[A\n",
            "275it [24:12,  5.98s/it]\u001b[A\n",
            "276it [24:13,  4.47s/it]\u001b[A\n",
            "277it [24:23,  6.32s/it]\u001b[A\n",
            "278it [24:24,  4.71s/it]\u001b[A\n",
            "279it [24:34,  6.18s/it]\u001b[A\n",
            "280it [24:35,  4.61s/it]\u001b[A\n",
            "281it [24:43,  5.54s/it]\u001b[A\n",
            "282it [24:44,  4.16s/it]\u001b[A\n",
            "283it [24:54,  6.01s/it]\u001b[A\n",
            "284it [24:55,  4.49s/it]\u001b[A\n",
            "285it [25:04,  5.79s/it]\u001b[A\n",
            "286it [25:05,  4.34s/it]\u001b[A\n",
            "287it [25:15,  6.22s/it]\u001b[A\n",
            "288it [25:16,  4.63s/it]\u001b[A\n",
            "289it [25:25,  5.96s/it]\u001b[A\n",
            "290it [25:26,  4.46s/it]\u001b[A\n",
            "291it [25:37,  6.29s/it]\u001b[A\n",
            "292it [25:38,  4.69s/it]\u001b[A\n",
            "293it [25:48,  6.28s/it]\u001b[A\n",
            "294it [25:49,  4.67s/it]\u001b[A\n",
            "295it [25:58,  5.97s/it]\u001b[A\n",
            "296it [25:59,  4.46s/it]\u001b[A\n",
            "297it [26:08,  5.84s/it]\u001b[A\n",
            "298it [26:09,  4.36s/it]\u001b[A\n",
            "299it [26:20,  6.35s/it]\u001b[A\n",
            "300it [26:21,  4.73s/it]\u001b[A\n",
            "301it [26:30,  6.22s/it]\u001b[A\n",
            "302it [26:31,  4.64s/it]\u001b[A\n",
            "303it [26:39,  5.71s/it]\u001b[A\n",
            "304it [26:40,  4.28s/it]\u001b[A\n",
            "305it [26:48,  5.16s/it]\u001b[A\n",
            "306it [26:48,  3.89s/it]\u001b[A\n",
            "307it [26:56,  4.90s/it]\u001b[A\n",
            "308it [26:57,  3.92s/it]\u001b[A\n",
            "309it [27:04,  4.77s/it]\u001b[A\n",
            "310it [27:08,  4.38s/it]\u001b[A\n",
            "311it [27:14,  5.12s/it]\u001b[A\n",
            "312it [27:19,  4.84s/it]\u001b[A\n",
            "313it [27:26,  5.49s/it]\u001b[A\n",
            "314it [27:28,  4.70s/it]\u001b[A\n",
            "315it [27:36,  5.53s/it]\u001b[A\n",
            "316it [27:38,  4.44s/it]\u001b[A\n",
            "317it [27:45,  5.33s/it]\u001b[A\n",
            "318it [27:48,  4.52s/it]\u001b[A\n",
            "319it [27:57,  5.90s/it]\u001b[A\n",
            "320it [27:58,  4.41s/it]\u001b[A\n",
            "321it [28:08,  6.14s/it]\u001b[A\n",
            "322it [28:09,  4.58s/it]\u001b[A\n",
            "323it [28:18,  5.97s/it]\u001b[A\n",
            "324it [28:19,  4.46s/it]\u001b[A\n",
            "325it [28:29,  6.22s/it]\u001b[A\n",
            "326it [28:30,  4.63s/it]\u001b[A\n",
            "327it [28:42,  6.63s/it]\u001b[A\n",
            "328it [28:43,  4.92s/it]\u001b[A\n",
            "329it [28:52,  6.39s/it]\u001b[A\n",
            "330it [28:53,  4.75s/it]\u001b[A\n",
            "331it [29:04,  6.40s/it]\u001b[A\n",
            "332it [29:05,  4.76s/it]\u001b[A\n",
            "333it [29:15,  6.57s/it]\u001b[A\n",
            "334it [29:16,  4.88s/it]\u001b[A\n",
            "335it [29:26,  6.24s/it]\u001b[A\n",
            "336it [29:27,  4.65s/it]\u001b[A\n",
            "337it [29:37,  6.40s/it]\u001b[A\n",
            "338it [29:38,  4.77s/it]\u001b[A\n",
            "339it [29:47,  6.00s/it]\u001b[A\n",
            "340it [29:48,  4.48s/it]\u001b[A\n",
            "341it [29:59,  6.43s/it]\u001b[A\n",
            "342it [30:00,  4.78s/it]\u001b[A\n",
            "343it [30:09,  6.23s/it]\u001b[A\n",
            "344it [30:10,  4.64s/it]\u001b[A\n",
            "345it [30:20,  6.08s/it]\u001b[A\n",
            "346it [30:21,  4.54s/it]\u001b[A\n",
            "347it [30:30,  5.99s/it]\u001b[A\n",
            "348it [30:31,  4.47s/it]\u001b[A\n",
            "349it [30:41,  6.14s/it]\u001b[A\n",
            "350it [30:42,  4.58s/it]\u001b[A\n",
            "351it [30:51,  6.02s/it]\u001b[A\n",
            "352it [30:52,  4.49s/it]\u001b[A\n",
            "353it [31:03,  6.25s/it]\u001b[A\n",
            "354it [31:04,  4.66s/it]\u001b[A\n",
            "355it [31:12,  5.81s/it]\u001b[A\n",
            "356it [31:13,  4.35s/it]\u001b[A\n",
            "357it [31:22,  5.74s/it]\u001b[A\n",
            "358it [31:23,  4.29s/it]\u001b[A\n",
            "359it [31:33,  6.11s/it]\u001b[A\n",
            "360it [31:34,  4.56s/it]\u001b[A\n",
            "361it [31:43,  5.88s/it]\u001b[A\n",
            "362it [31:44,  4.40s/it]\u001b[A\n",
            "363it [31:52,  5.41s/it]\u001b[A\n",
            "364it [31:53,  4.07s/it]\u001b[A\n",
            "365it [32:03,  5.85s/it]\u001b[A\n",
            "366it [32:04,  4.37s/it]\u001b[A\n",
            "367it [32:14,  6.22s/it]\u001b[A\n",
            "368it [32:15,  4.64s/it]\u001b[A\n",
            "369it [32:25,  6.23s/it]\u001b[A\n",
            "370it [32:26,  4.64s/it]\u001b[A\n",
            "371it [32:36,  6.13s/it]\u001b[A\n",
            "372it [32:37,  4.57s/it]\u001b[A\n",
            "373it [32:47,  6.19s/it]\u001b[A\n",
            "374it [32:48,  4.61s/it]\u001b[A\n",
            "375it [32:55,  5.60s/it]\u001b[A\n",
            "376it [32:56,  4.20s/it]\u001b[A\n",
            "377it [33:05,  5.62s/it]\u001b[A\n",
            "378it [33:06,  4.21s/it]\u001b[A\n",
            "379it [33:16,  5.85s/it]\u001b[A\n",
            "380it [33:17,  4.38s/it]\u001b[A\n",
            "381it [33:28,  6.48s/it]\u001b[A\n",
            "382it [33:29,  4.82s/it]\u001b[A\n",
            "383it [33:40,  6.53s/it]\u001b[A\n",
            "384it [33:41,  4.85s/it]\u001b[A\n",
            "385it [33:51,  6.55s/it]\u001b[A\n",
            "386it [33:52,  4.86s/it]\u001b[A\n",
            "387it [34:03,  6.67s/it]\u001b[A\n",
            "388it [34:04,  4.95s/it]\u001b[A\n",
            "389it [34:14,  6.47s/it]\u001b[A\n",
            "390it [34:15,  4.81s/it]\u001b[A\n",
            "391it [34:24,  6.13s/it]\u001b[A\n",
            "392it [34:25,  4.57s/it]\u001b[A\n",
            "393it [34:35,  6.15s/it]\u001b[A\n",
            "394it [34:36,  4.59s/it]\u001b[A\n",
            "395it [34:45,  6.12s/it]\u001b[A\n",
            "396it [34:46,  4.57s/it]\u001b[A\n",
            "397it [34:56,  5.97s/it]\u001b[A\n",
            "398it [34:57,  4.46s/it]\u001b[A\n",
            "399it [35:07,  6.25s/it]\u001b[A\n",
            "400it [35:08,  4.65s/it]\u001b[A\n",
            "401it [35:17,  5.89s/it]\u001b[A\n",
            "402it [35:18,  4.40s/it]\u001b[A\n",
            "403it [35:27,  5.83s/it]\u001b[A\n",
            "404it [35:28,  4.36s/it]\u001b[A\n",
            "405it [35:38,  6.08s/it]\u001b[A\n",
            "406it [35:39,  4.54s/it]\u001b[A\n",
            "407it [35:49,  6.38s/it]\u001b[A\n",
            "408it [35:50,  4.75s/it]\u001b[A\n",
            "409it [35:59,  5.82s/it]\u001b[A\n",
            "410it [36:00,  4.35s/it]\u001b[A\n",
            "411it [36:09,  5.83s/it]\u001b[A\n",
            "412it [36:10,  4.36s/it]\u001b[A\n",
            "413it [36:19,  5.74s/it]\u001b[A\n",
            "414it [36:20,  4.30s/it]\u001b[A\n",
            "415it [36:29,  5.82s/it]\u001b[A\n",
            "416it [36:30,  4.36s/it]\u001b[A\n",
            "417it [36:39,  5.62s/it]\u001b[A\n",
            "418it [36:40,  4.22s/it]\u001b[A\n",
            "419it [36:49,  5.78s/it]\u001b[A\n",
            "420it [36:50,  4.33s/it]\u001b[A\n",
            "421it [36:59,  5.63s/it]\u001b[A\n",
            "422it [37:00,  4.22s/it]\u001b[A\n",
            "423it [37:10,  6.07s/it]\u001b[A\n",
            "424it [37:11,  4.53s/it]\u001b[A\n",
            "425it [37:21,  6.16s/it]\u001b[A\n",
            "426it [37:22,  4.59s/it]\u001b[A\n",
            "427it [37:33,  6.56s/it]\u001b[A\n",
            "428it [37:34,  4.87s/it]\u001b[A\n",
            "429it [37:42,  5.81s/it]\u001b[A\n",
            "430it [37:43,  4.35s/it]\u001b[A\n",
            "431it [37:53,  6.13s/it]\u001b[A\n",
            "432it [37:54,  4.58s/it]\u001b[A\n",
            "433it [38:04,  6.09s/it]\u001b[A\n",
            "434it [38:05,  4.54s/it]\u001b[A\n",
            "435it [38:13,  5.77s/it]\u001b[A\n",
            "436it [38:14,  4.32s/it]\u001b[A\n",
            "437it [38:22,  5.38s/it]\u001b[A\n",
            "438it [38:23,  4.04s/it]\u001b[A\n",
            "439it [38:33,  5.92s/it]\u001b[A\n",
            "440it [38:34,  4.42s/it]\u001b[A\n",
            "441it [38:44,  5.96s/it]\u001b[A\n",
            "442it [38:45,  4.45s/it]\u001b[A\n",
            "443it [38:53,  5.74s/it]\u001b[A\n",
            "444it [38:54,  4.30s/it]\u001b[A\n",
            "445it [39:05,  6.11s/it]\u001b[A\n",
            "446it [39:06,  4.56s/it]\u001b[A\n",
            "447it [39:15,  5.98s/it]\u001b[A\n",
            "448it [39:16,  4.46s/it]\u001b[A\n",
            "449it [39:24,  5.66s/it]\u001b[A\n",
            "450it [39:25,  4.24s/it]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7sSm3CHN1_T"
      },
      "source": [
        "auc, f1 = get_auc_f1(model, test_dataset)\n",
        "fd = os.open(oj(dir_path, \"auc_f1_299.txt\"), os.O_RDWR|os.CREAT)\n",
        "os.write(fd, \"AUC: \" + str(auc))\n",
        "os.write(fd, \"F1: \" + str(f1))\n",
        "os.close(fd)w"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}